{"trial_id": "0012", "hyperparameters": {"space": [{"class_name": "Int", "config": {"name": "conv_1_filter", "default": null, "conditions": [], "min_value": 32, "max_value": 96, "step": 16, "sampling": "linear"}}, {"class_name": "Int", "config": {"name": "conv_2_filter", "default": null, "conditions": [], "min_value": 32, "max_value": 96, "step": 16, "sampling": "linear"}}, {"class_name": "Int", "config": {"name": "conv_3_filter", "default": null, "conditions": [], "min_value": 32, "max_value": 128, "step": 16, "sampling": "linear"}}, {"class_name": "Int", "config": {"name": "dense_units", "default": null, "conditions": [], "min_value": 128, "max_value": 256, "step": 32, "sampling": "linear"}}, {"class_name": "Float", "config": {"name": "dropout", "default": 0.2, "conditions": [], "min_value": 0.2, "max_value": 0.5, "step": 0.1, "sampling": "linear"}}, {"class_name": "Choice", "config": {"name": "learning_rate", "default": 0.01, "conditions": [], "values": [0.01, 0.001, 0.0001], "ordered": true}}], "values": {"conv_1_filter": 64, "conv_2_filter": 96, "conv_3_filter": 96, "dense_units": 192, "dropout": 0.30000000000000004, "learning_rate": 0.001, "tuner/epochs": 4, "tuner/initial_epoch": 0, "tuner/bracket": 1, "tuner/round": 0}}, "metrics": {"metrics": {}}, "score": null, "best_step": 0, "status": "FAILED", "message": "Traceback (most recent call last):\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/tuners/hyperband.py\", line 427, in run_trial\n    return super().run_trial(trial, *fit_args, **fit_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n    except TypeError as e:\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/pnamy/Projeto_Final_v13/backend/models/age_gender_race/train_model.py\", line 316, in <module>\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/tuners/hyperband.py\", line 427, in run_trial\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 314, in fit\n\n  File \"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 117, in one_step_on_iterator\n\nOut of memory while trying to allocate 4019767752 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:   26.18MiB\n              constant allocation:         4B\n        maybe_live_out allocation:    2.16MiB\n     preallocated temp allocation:    3.74GiB\n  preallocated temp fragmentation:   97.27MiB (2.54%)\n                 total allocation:    3.77GiB\n              total fragmentation:   97.48MiB (2.53%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 768.00MiB\n\t\tOperator: op_type=\"MaxPoolGrad\" op_name=\"gradient_tape/functional_1_1/max_pooling2d_1/MaxPool2d/MaxPoolGrad\" source_file=\"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1177\n\t\tXLA Label: select-and-scatter\n\t\tShape: f32[32,256,256,96]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 768.00MiB\n\t\tXLA Label: fusion\n\t\tShape: f32[32,256,256,96]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 768.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_1_2/convolution\" source_file=\"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1177\n\t\tXLA Label: custom-call\n\t\tShape: f32[32,96,256,256]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_1/convolution\" source_file=\"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1177\n\t\tXLA Label: custom-call\n\t\tShape: f32[32,64,256,256]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"AddV2\" op_name=\"functional_1_1/batch_normalization_1/batchnorm/add_1\" source_file=\"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1177\n\t\tXLA Label: fusion\n\t\tShape: f32[32,64,256,256]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 192.00MiB\n\t\tOperator: op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/home/pnamy/anaconda3/envs/py312/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1177\n\t\tXLA Label: custom-call\n\t\tShape: f32[32,96,128,128]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 192.00MiB\n\t\tXLA Label: fusion\n\t\tShape: f32[32,96,128,128]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 24.00MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tXLA Label: fusion\n\t\tShape: f32[32,3,256,256]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 24.00MiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tEntry Parameter Subshape: f32[32,256,256,3]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 324.0KiB\n\t\tOperator: op_name=\"XLA_Retvals\"\n\t\tXLA Label: fusion\n\t\tShape: f32[3,3,96,96]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 324.0KiB\n\t\tOperator: op_name=\"XLA_Retvals\"\n\t\tXLA Label: fusion\n\t\tShape: f32[3,3,96,96]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 324.0KiB\n\t\tOperator: op_name=\"XLA_Retvals\"\n\t\tXLA Label: fusion\n\t\tShape: f32[3,3,96,96]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 216.0KiB\n\t\tOperator: op_name=\"XLA_Args\"\n\t\tXLA Label: fusion\n\t\tShape: f32[96,64,3,3]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 216.0KiB\n\t\tOperator: op_name=\"XLA_Retvals\"\n\t\tXLA Label: fusion\n\t\tShape: f32[3,3,64,96]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 216.0KiB\n\t\tOperator: op_name=\"XLA_Retvals\"\n\t\tXLA Label: fusion\n\t\tShape: f32[3,3,64,96]\n\t\t==========================\n\n\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_14336]\n"}